---
title: "Lecture 4 Code"
author: "Alex Hoagland"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r header}
################################################################################
# I like to include several additional notes in the header of my files here:
#
# Last modified: 8/11/2022
#
### PURPOSE:
  # Lecture 3 code and output file
# 
### NOTES:
  # - uses the Tidyverse package and Dplyr
################################################################################


### 0. Load the packages we will need for this file ####
library(tidyverse) # load the installed package for each new session of R
library(broom)
library(faux) # Useful package for simulating data
library(modelsummary) # For making regression tables
library(causaldata) # Useful toy data sets
library(here) # Helpful in working with directories and projects

set.seed(03262020) # Setting the seed helps to make random number generators give us the same numbers across machines
################################################################################
```

## Effect of an outlier in a regression

Even a single outlier can change your regression by a lot! This is why nonlinear transformations can be so useful. This code creates an animation to show the effect of including/excluding a single outlier on a regression:

```{r animate-outliers}
library(gganimate)

# Main data
animatedata <- data.frame(x=rnorm(n=100,mean=5,sd=2))
animatedata <- animatedata %>% mutate(y=-.5*x+rnorm(n=100,mean=0,sd=.25))

# Duplicate each row (for transition in animation)
animatedata <- animatedata %>% bind_rows(animatedata)
animatedata$transition <- 0
animatedata[101:200,]$transition <- 1

# Add (one big) outlier to the second set of data 
animatedata[201,] <- c(20,-20,1) 

# Scatter plot with outlier
ggplot(animatedata,aes(x=x,y=y)) + geom_point() + theme_classic()

# Scatter plot without outlier
animatedata %>% filter(transition == 0) %>% ggplot(aes(x=x,y=y)) + geom_point() + theme_classic()

# To make animated data, need to duplicate rows for all data points in both states (add/subtract only outlier)

animfig <- ggplot(animatedata,aes(x=x,y=y)) + 
  geom_point() + 
  geom_smooth(data=animatedata,aes(x=x,y=y),method='lm') + 
  # Here comes the gganimate code
  transition_states(
    transition,
    transition_length = 1,
    state_length = 1
  ) +
  enter_fade() + 
  exit_shrink() +
  ease_aes('linear') + 
  theme_classic()
animate(animfig)
anim_save(filename=here("RegressionOutlier.gif"),animation=animfig)
```

## Interaction Terms

Our question will be: what is the effect of education on wages? We can estimate a simple regression controlling only for education and experience -- for now, assume that this is a fully specified model.

```{r mydata}
mydata <- causaldata::close_college

# First pass: simple regression in levels
mydata <- mydata %>% mutate(wage = exp(lwage))
m_levels <- lm(wage ~ educ + exper, data=mydata)

m1 <- lm(lwage ~ educ + exper, data=mydata)
msummary(list("levels"=m_levels, "log_y"=m1),
         stars=c('*' = .1, '**' = .05, '***' = .01)) 
```

But what if we think the returns to education will differ across race? Let's use an interaction model to examine heterogeneous treatment effects here. Recall that in an interaction model we need to include **all level terms** of our variables of interest, not just the interactions.

```{r interactions}
# How does the effect of education differ across race?
m2 <- lm(lwage ~ educ + exper + black + educ:black, data=mydata)
msummary(list(m1,m2),
         stars=c('*' = .1, '**' = .05, '***' = .01)) 

# What if we had left out the main effect?
m3 <- lm(lwage ~ educ + exper + educ:black, data=mydata)
msummary(list(m1,m2,m3),
         stars=c('*' = .1, '**' = .05, '***' = .01)) 
```

## Polynomial Models

What if we think experience has a nonlinear effect on wages? Why might this be true?

Let's model this nonlinearity by including a squared term in the regression

```{r nonlinear}
ggplot(data=mydata,aes(x=exper,y=lwage)) + geom_point() + 
  theme_minimal() + labs(x="Experience (Years)",y="Wages") + 
  geom_smooth(method='lm', formula= y~x)

# What if we add in a squared term? 
ggplot(data=mydata,aes(x=exper,y=lwage)) + geom_point() + 
  theme_minimal() + labs(x="Experience (Years)",y="Wages") + 
  stat_smooth(method="lm", se=TRUE, fill=NA,
              formula=y ~ poly(x, 2, raw=TRUE),colour="red")
```

How do our results change if we increase the order of the polynomial we fit? Why should/shouldn't we do this?

```{r higher-order}
# What about a cubic term? 
ggplot(data=mydata,aes(x=exper,y=lwage)) + geom_point() + 
  theme_minimal() + labs(x="Experience (Years)",y="Wages") + 
  stat_smooth(method="lm", se=TRUE, fill=NA,
              formula=y ~ poly(x, 3, raw=TRUE),colour="red")

# Higher order terms? 
n <- 10 # You pick a polynomial
ggplot(data=mydata,aes(x=exper,y=lwage)) + geom_point() + 
  theme_minimal() + labs(x="Experience (Years)",y="Wages") + 
  stat_smooth(method="lm", se=TRUE, fill=NA,
              formula=y ~ poly(x, n, raw=TRUE),colour="red")
```

## Standard Errors

### Heteroskedasticity

Let's check if our assumption of homoskedasticity is satisfied.

```{r plot-variance}
mydata <- mydata %>% mutate(wage=exp(lwage))
ggplot(mydata,aes(x=educ,y=wage)) + geom_point() + theme_minimal()
```

Clearly, this assumption fails. Let's implement robust standard errors in our regressions

```{r robust-ses}
m1 <- lm(lwage ~ educ + exper + black + married + nearc4 + educ:black + exper:black, data=mydata)
msummary(list("naive"=m1),
         stars=c('*' = .1, '**' = .05, '***' = .01))

# Same model with robust standard errors
msummary(list("naive"=m1,"robust"=m1),
         vcov=c("classical","robust"),
         stars=c('*' = .1, '**' = .05, '***' = .01))
```

### Clustering

Now, suppose that we have data on geography (US State). What if the returns to education/experience are correlated within states in unique ways? Why might this be true?

```{r clustered-ses}
mydata$state <- unlist(causaldata::abortion[1:3010,1])
m1 <- lm(lwage ~ educ + exper + black + married + nearc4 + educ:black + exper:black, data=mydata)

msummary(list("naive"=m1,"robust"=m1,"Clustered"=m1),
         vcov=c("classical","robust",~state),
         stars=c('*' = .1, '**' = .05, '***' = .01))
```

### Bootstrapping standard errors

For whatever reason, you may need to bootstrap your standard errors. This is a sample code for how you might do that.

```{r bootstrap}
numiter <- 1000 # Number of times you want to sample
sizesample=nrow(mydata) # Size of the sample you want to take

# Create a place to store the coefficients
allbetas <- rep(NA,numiter)

# Run the loop
for (i in 1:numiter) { 
  sampledata <- mydata[sample(nrow(mydata), sizesample,replace=T), ]
  samplemodel <- lm(lwage ~ educ + exper + black + married + nearc4 + educ:black + exper:black, data=sampledata)
  allbetas[i] <- samplemodel$coefficients[2] # Effect of education on lwage
}

# Represent graphically
allbetas <- tibble(allbetas)
ggplot(allbetas,aes(x=allbetas)) + geom_histogram(fill='gray',color="black") + theme_minimal() + 
  labs(x="Estimated Betas",y="Count") + geom_vline(xintercept=0.077,color="red",size=2)

sd(allbetas$allbetas) # This gives us a bootstrapped SE remarkably similar to the regression approach
```

## Package Citations

```{r, include=FALSE}
print("=============================Works Cited=============================")
loadedNamespaces() %>%
map(citation) %>%
print(style = "text") # Adds citations for each package to end of .rmd file

knitr::write_bib(file = 'packages.bib') # Constructs a citation file for all packages used in this lecture.

# DON'T FORGET TO CITE YOUR PACKAGES IN YOUR PAPERS/ASSIGNMENTS. 
```
